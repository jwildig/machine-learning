{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Neural Network by Backpropagation\n",
    "\n",
    "In this document, I build and train a neural network on a number of synthetic datasets, visualising the decision boundary of the network at regular intervals during training. The goal of this exercise is to demonstrate that neural networks can learn highly non-linear decision boundaries.\n",
    "\n",
    "Firstly, packages relevant to the datasets and plotting are imported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a dataset containing points on the plane arranged in two opposing crescent shapes, called moons, is created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_moons, y_moons = datasets.make_moons(1000, noise = 0.15)\n",
    "\n",
    "# Rescale points to be centered about (0,0)\n",
    "X_moons = X_moons - np.amin(X_moons, axis = 0)\n",
    "X_moons = X_moons/np.amax(X_moons, axis = 0)*1.5 - 0.75\n",
    "y_moons = y_moons.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next visualise the dataset using matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.set_cmap('jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_moons[:,0],X_moons[:,1], c=y_moons.ravel(), s = 1)\n",
    "plt.axis('square');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataset containing points in two classes is created: one approximately within a disc surrounding the origin and one approximately outside the disc. The class of the point is decided using a distribution which is a function of the distance of the point from the origin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 8\n",
    "sigma = 0.7\n",
    "X_disc = np.random.rand(1000, 2)*2-1\n",
    "y_disc = np.exp(-(np.linalg.norm(X_disc, axis = 1)/sigma)**beta).reshape(-1,1) > np.random.rand(1000,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_disc[:,0], X_disc[:,1], c=y_disc.ravel(), s = 1)\n",
    "plt.axis('square');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid function is introduced to act as the activation function for the neurons in the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of the sigmoid function is used in the backpropagation step of training the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two functions serve to store and extract parameters from a dictionary for ease of passing into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_parameters(W1, b1, W2, b2):\n",
    "    return {\"W1\": W1, \"W2\": W2, \"b1\": b1, \"b2\": b2}\n",
    "\n",
    "def unpack_parameters(parameters):\n",
    "    return [parameters[\"W1\"], parameters[\"b1\"],parameters[\"W2\"],parameters[\"b2\"]]\n",
    "\n",
    "def unpack_cache(cache):\n",
    "    return [cache[\"Z1\"],cache[\"A1\"],cache[\"Z2\"],cache[\"A2\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward propagation function takes in a set of points and a two-layer neural network and returns the network's predictions and a cache of the activations of the network after propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagate(X, parameters, threshold = 0.5):\n",
    "    W1, b1, W2, b2 = unpack_parameters(parameters)\n",
    "    Z1 = np.dot(X, W1) + b1.T\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    cache = {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n",
    "    return (A2 > threshold), cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function J is minimized through training the neural network by backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J(pred, y):\n",
    "    m = len(pred)\n",
    "    return 1/(2*m)*np.sum(np.power((y - pred),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network parameters are initialised as follows: the weights to small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_parameters(hidden, epsilon = 0.01):\n",
    "    W1 = np.random.randn(2, hidden) * epsilon\n",
    "    W2 = np.random.randn(hidden, 1) * epsilon\n",
    "    b1 = np.zeros((hidden, 1))\n",
    "    b2 = 0\n",
    "    \n",
    "    return([W1, b1, W2, b2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A grid is created over the the range of the datapoints, so that a decision boundary can be displayed while the neural network is learning. The grid is passed through the network and the resulting response visualised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgrid, ygrid = np.meshgrid(np.linspace(-1, 1, 100),np.linspace(-1, 1, 100))\n",
    "Xgrid = np.vstack([xgrid.ravel(), ygrid.ravel()]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(X, y, parameters, cost):\n",
    "    \n",
    "    pred_grid, _ = forward_propagate(Xgrid, parameters)\n",
    "    display.clear_output(True)\n",
    "\n",
    "    plt.contour(xgrid ,ygrid, pred_grid.reshape(xgrid.shape))\n",
    "    plt.axis('equal')\n",
    "    plt.hold(True)\n",
    "    plt.scatter(X[:,0],X[:,1], c=y.ravel(), s = 1)\n",
    "    plt.hold(False)\n",
    "    plt.title(\"Cost function J = {:.6f}\".format(cost))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function trains the neural network while intermittently calling the plot_decision_boundary function to visualise the progress of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, hidden = 3, epochs = 20000, alpha = 10, epsilon = 0.1):\n",
    "\n",
    "    W1, b1, W2, b2 = initialise_parameters(hidden, epsilon)\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        parameters = pack_parameters(W1, b1, W2, b2)\n",
    "        pred_moons, cache = forward_propagate(X, parameters)\n",
    "\n",
    "        Z1, A1, Z2, A2 = unpack_cache(cache)\n",
    "        \n",
    "        # Plot the decision boundary once every n epochs\n",
    "        if np.mod(i, 250) == 0:\n",
    "            Jval = J(A2, y)\n",
    "            plot_decision_boundary(X, y, parameters, Jval)\n",
    "        \n",
    "        # Calculate the derivative of the cost function with respect to the parameters\n",
    "        dy  = (A2 - y)\n",
    "        db2 = sigmoid_derivative(Z2) * dy\n",
    "        dW2 = A1 * db2\n",
    "        db1 = sigmoid_derivative(Z1) * dW2 * W2.T\n",
    "        dW1 = np.dot(X.T,  db1)\n",
    "        \n",
    "        # Update the parameters according to the calculated derivatives and the learning rate\n",
    "        b2 = b2 - alpha * 1/m*np.sum(db2, axis = 0, keepdims = True)\n",
    "        W2 = W2 - alpha * 1/m*np.sum(dW2, axis = 0, keepdims = True).T\n",
    "        b1 = b1 - alpha * 1/m*np.sum(db1, axis = 0, keepdims = True).T\n",
    "        W1 = W1 - alpha * 1/m*dW1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We attempt to train the model on the 'moons' dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(X_moons, y_moons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the 'disc' dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(X_disc, y_disc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Neural networks can effectively learn non-linear decision boundaries, and can be implemented in Python using relatively few lines of code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
